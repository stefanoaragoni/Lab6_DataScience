{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laboratorio 6 - Generative Adversarial Network\n",
    "Stefano Aragoni, Carol Arévalo\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica se diseñó una Generative Adversarial Network (GAN) con el propósito de poder generar imágenes artificiales que imiten la distribución de los datos originales Para esto, fue necesario diseñar una red neuronal que fuera capaz de generar imágenes, y otra red neuronal que fuera capaz de diferenciar entre imágenes reales y generadas. \n",
    "\n",
    "A continuación se muestra el código utilizado para la creación de la GAN, así como los resultados obtenidos.\n",
    "\n",
    "------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importar librerías\n",
    "\n",
    "Como primer paso, se importaron las librerías necesarias para la creación de la GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Reshape, Dropout, LeakyReLU, Flatten, BatchNormalization, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "import pickle\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### **Preparación de Datos**\n",
    "\n",
    "##### Cargar el dataset de CelebA y Preprocesamiento de Datos \n",
    "\n",
    "Para iniciar, se descargó el dataset de CelebA. Este conjunto de datos consta de más de 200,000 imágenes a color, de 128 X 128 X 3 c/u. A continuación se muestra la ubicación de las imágenes en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599 fotos en el directorio\n"
     ]
    }
   ],
   "source": [
    "# Direcciones de los archivos\n",
    "fotos_dir = 'archive/img_align_celeba/'\n",
    "fotos_dir_class = 'archive/img_align_celeba/img_align_celeba/'\n",
    "\n",
    "# Cantidad de fotos en el directorio\n",
    "n_fotos = len(os.listdir(fotos_dir_class))\n",
    "print(n_fotos, \"fotos en el directorio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de la librería de Keras, se cargaron las imágenes por batches.\n",
    "\n",
    "Asimismo, se les aplicó un preprocesamiento, el cual consistió en normalizar los valores de los pixeles de las imágenes, recortarlas y redimensionarlas a 64 X 64 X 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Tamaño de las imágenes\n",
    "img_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer las imagenes -> Función recomendada por Prof. Luis Furlan.\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,                    # PRE-PROCESAMIENTO: Normalizar los valores de los pixeles\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Carga las imagenes de entrenamiento\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    fotos_dir,\n",
    "    target_size=(img_size, img_size),        # PRE-PROCESAMIENTO: RECORTAR Y REDIMENSIONAR IMÁGENES\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### **Implementación de la GAN**\n",
    "\n",
    "##### Diseño del generador y el discriminador\n",
    "\n",
    "A continuación se demuestra el modelo del <font color=orange>generador</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generador = Sequential()\n",
    "generador.add(Dense(8 * 8 * 256, input_shape=[tamanio_codificacion])) \n",
    "generador.add(Reshape([8, 8, 256]))\n",
    "generador.add(BatchNormalization())\n",
    "generador.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"))  \n",
    "generador.add(BatchNormalization())\n",
    "generador.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"))  \n",
    "generador.add(BatchNormalization())\n",
    "generador.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding=\"same\", activation=\"sigmoid\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se demuestra el modelo del <font color=orange>discriminador</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminador = Sequential()\n",
    "discriminador.add(Conv2D(64, kernel_size=5, strides=2, padding=\"same\", input_shape=(64, 64, 3)))\n",
    "discriminador.add(LeakyReLU(0.3))\n",
    "discriminador.add(Dropout(0.5))\n",
    "discriminador.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminador.add(LeakyReLU(0.3))\n",
    "discriminador.add(Dropout(0.5))\n",
    "discriminador.add(Flatten())\n",
    "discriminador.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definición de funciones de pérdida y optimizadores\n",
    "\n",
    "Con los modelos listos, se procedió a definir las funciones de pérdida y los optimizadores. Esto con el propósito de poder entrenar la GAN. Más específicamente, se utilizó la función de <font color=orange>Binary Cross Entropy</font> (BCE) como función de pérdida, y el <font color=orange>optimizador Adam</font>. \n",
    "\n",
    "Asimismo, se indicó que el discriminador no se entrenaría durante el entrenamiento de la GAN, ya que el objetivo es entrenar al generador para que engañe al discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN = Sequential([generador, discriminador])\n",
    "\n",
    "discriminador.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "discriminador.trainable = False\n",
    "\n",
    "GAN.compile(loss = \"binary_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### **Entrenamiento de la GAN**\n",
    "\n",
    "##### Implementación del bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se desea que el entrenamiento sea más\n",
    "#   rápido, se puede tomar un valor mayor\n",
    "tamanio_tanda = 32\n",
    "\n",
    "# mis_datos = X_entreno\n",
    "mis_datos = train_generator[0]\n",
    "\n",
    "datos = tf.data.Dataset.from_tensor_slices(mis_datos).shuffle(buffer_size = 1000)\n",
    "\n",
    "datos = datos.batch(tamanio_tanda, \n",
    "                    drop_remainder = True).prefetch(1)\n",
    "\n",
    "epocas = 20\n",
    "\n",
    "\n",
    "# Tomar los componentes por separado\n",
    "generador, discriminador = GAN.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [32,28,28,1] vs. shape[1] = [32,64,64,3] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/stefanoaragoni/Documents/GitHub/Lab6_DataScience/lab6.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefanoaragoni/Documents/GitHub/Lab6_DataScience/lab6.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m generated_images \u001b[39m=\u001b[39m generator(noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefanoaragoni/Documents/GitHub/Lab6_DataScience/lab6.ipynb#X51sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Concatenate fake and real images\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stefanoaragoni/Documents/GitHub/Lab6_DataScience/lab6.ipynb#X51sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m X_fake_vs_real \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconcat([generated_images, X_batch], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefanoaragoni/Documents/GitHub/Lab6_DataScience/lab6.ipynb#X51sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Set targets: 0 for fake images, 1 for real images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stefanoaragoni/Documents/GitHub/Lab6_DataScience/lab6.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m y1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([[\u001b[39m0.\u001b[39m]] \u001b[39m*\u001b[39m batch_size \u001b[39m+\u001b[39m [[\u001b[39m1.\u001b[39m]] \u001b[39m*\u001b[39m batch_size)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [32,28,28,1] vs. shape[1] = [32,64,64,3] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "\n",
    "# Number of epochs for training\n",
    "epochs = 20\n",
    "\n",
    "# Generator and discriminator components\n",
    "generator, discriminator = GAN.layers\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through batches in the training generator\n",
    "    for X_batch in train_generator:\n",
    "        i += 1\n",
    "        if i % 25 == 0:\n",
    "            print(f\"\\tBatch {i} of {len(train_generator)}\")\n",
    "            \n",
    "        # Fase 1 - Entrenamiento del DISCRIMINADOR\n",
    "        \n",
    "        # Create noise\n",
    "        noise = tf.random.normal(shape=[batch_size, tamanio_codificacion])\n",
    "        \n",
    "        # Generate fake images based on noise\n",
    "        generated_images = generator(noise)\n",
    "        \n",
    "        # Concatenate fake and real images\n",
    "        X_fake_vs_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "        \n",
    "        # Set targets: 0 for fake images, 1 for real images\n",
    "        y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "        \n",
    "        # Allow the discriminator to be trainable\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Train the discriminator on this batch\n",
    "        discriminator.train_on_batch(X_fake_vs_real, y1)\n",
    "        \n",
    "        # Fase 2 - Entrenamiento del GENERADOR\n",
    "        \n",
    "        # Create some noise\n",
    "        noise = tf.random.normal(shape=[batch_size, tamanio_codificacion])\n",
    "        \n",
    "        # Make the discriminator believe that the fake images are real\n",
    "        y2 = tf.constant([[1.]] * batch_size)\n",
    "        \n",
    "        # Prevent a warning\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        # Train the GAN (generator) on this batch\n",
    "        GAN.train_on_batch(noise, y2)\n",
    "\n",
    "print(\"Training Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruido = tf.random.normal(shape = [10, tamanio_codificacion])\n",
    "\n",
    "imagenes = generador(ruido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imagenes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualización de los resultados durante el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### **Reflexión**\n",
    "\n",
    "Reflexione sobre lo aprendido en la sesión teórica y cómo se aplicó en el laboratorio. Algunos \n",
    "puntos que podrían considerar en su reflexión incluyen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué conceptos de la teoría encontraron más desafiantes y por qué?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Cómo les ayudó el laboratorio a consolidar o entender mejor estos conceptos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Qué aplicaciones potenciales ven para las GANs en la industria o en la investigación?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Qué limitaciones o preocupaciones éticas pueden identificar en el uso de GANs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Cómo se sienten con respecto a la implementación y entrenamiento de GANs después de la  experiencia práctica?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
